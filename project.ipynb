{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  QID  Site_id                                           Question  \\\n",
      "0   1        0                لماذا الرجال أكثر وزناً بعد الزواج؟   \n",
      "1   2        1           لماذا تخشى عصابات المافيا الكلاب في دبي؟   \n",
      "2   3        2  لماذا يختلف تعامل الرجل والمرأة مع قصص الحب ال...   \n",
      "3   4        3        لماذا تسعى الإمارات لاستكشاف الكوكب الأحمر؟   \n",
      "4   5        4                          لماذا سقط الإخوان في مصر؟   \n",
      "\n",
      "                                            Document  \\\n",
      "0  \"كشفت دراسة علمية عن أنّ الرجال المتزوِّجين يك...   \n",
      "1  \"لا تزال عصابات تهريب المخدرات، وكل أنواع المم...   \n",
      "2  \"تقضي الكثير من النساء أسابيع وشهورا في استعاد...   \n",
      "3  المريخ، الكوكب الأحمر، ثاني أقرب الكواكب إلينا...   \n",
      "4  \"سقط الإخوان في مصر بأسرع مما يتوقع أو يتخيل خ...   \n",
      "\n",
      "                                             Answer1  \\\n",
      "0  \"أن من يخوضون علاقات، يتناولون الطعام بدرجة أكبر\"   \n",
      "1  \"مدربة على أعلى مستوى وتتحدى أخطر عصابات التهر...   \n",
      "2  \"وفي الوقت الذي تحاول فيه المرأة لشهور طويلة م...   \n",
      "3  \"وذلك لأنّ دراسة الغلاف الجوي للكوكب الأحمر ست...   \n",
      "4  \"ولهذا السقوط أسباب متعددة، ومعظمها يعود إلى ط...   \n",
      "\n",
      "                                             Answer2  \\\n",
      "0  \"بسبب الالتزامات الاجتماعية التي قد تنشأ بعد ا...   \n",
      "1                                                NaN   \n",
      "2  \"فمشاعر الغضب والقلق والخوف هي التي كانت سائدة...   \n",
      "3  \"لتطوير كبير في كافة المجالات المختلفة، منها ا...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             Answer3  \\\n",
      "0  \"ولكنّهم يبدأون في تناول الأطعمة الأقل صحية، و...   \n",
      "1                                                NaN   \n",
      "2  \"هوالطبيعة البيولوجية لكل منهما فالمرأة تستثمر...   \n",
      "3  مما يعزز مكانتها العلميّة ويساعدها في بناء اقت...   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             Answer4 Answer5 Answer6 Answer7  \\\n",
      "0                                                NaN     NaN     NaN     NaN   \n",
      "1                                                NaN     NaN     NaN     NaN   \n",
      "2  \"أن طبيعة المرأة تجعلها انتقائية في اختيار الر...     NaN     NaN     NaN   \n",
      "3                                                NaN     NaN     NaN     NaN   \n",
      "4                                                NaN     NaN     NaN     NaN   \n",
      "\n",
      "  Answer8 Answer9 Answer10 Answer11   Original_Category Author  \\\n",
      "0     NaN     NaN      NaN      NaN        البيان الصحي    NaN   \n",
      "1     NaN     NaN      NaN      NaN  الابتكار التفاعلي     NaN   \n",
      "2     NaN     NaN      NaN      NaN         دويتشه فيله    NaN   \n",
      "3     NaN     NaN      NaN      NaN           الاقتصادي    NaN   \n",
      "4     NaN     NaN      NaN      NaN             اتجاهات    NaN   \n",
      "\n",
      "             Date     Site  Year  \n",
      "0   08 أبريل 2018  Albayan  2018  \n",
      "1  05 أكتوبر 2015  Albayan   NaN  \n",
      "2   12 أغسطس 2015  Albayan   NaN  \n",
      "3   19 يوليو 2016  Albayan   NaN  \n",
      "4   01 أغسطس 2013  Albayan  2013  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Excel file\n",
    "xlsx_file_path = 'QA.xlsx'\n",
    "\n",
    "# CSV file\n",
    "csv_file_path = 'QA.csv'\n",
    "\n",
    "# Read the Excel file into a dataframe \n",
    "df_xlsx = pd.read_excel(xlsx_file_path)\n",
    "\n",
    "# Save the dataframe as csv \n",
    "df_xlsx.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Print the first 5 rows of the dataframe\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the names of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "QID\n",
      "Site_id\n",
      "Question\n",
      "Document\n",
      "Answer1\n",
      "Answer2\n",
      "Answer3\n",
      "Answer4\n",
      "Answer5\n",
      "Answer6\n",
      "Answer7\n",
      "Answer8\n",
      "Answer9\n",
      "Answer10\n",
      "Answer11\n",
      "Original_Category\n",
      "Author\n",
      "Date\n",
      "Site\n",
      "Year\n"
     ]
    }
   ],
   "source": [
    "# List the names of the columns\n",
    "column_names = df.columns.tolist()\n",
    "\n",
    "# Print the column names\n",
    "print(\"Column names:\")\n",
    "for column_name in column_names:\n",
    "    print(column_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing important libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import ISRIStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [كشفت, دراسة, علمية, عن, أن, الرجال, المتزو, ج...\n",
      "1    [لا, تزال, عصابات, تهريب, المخدرات, وكل, أنواع...\n",
      "2    [تقضي, الكثير, من, النساء, أسابيع, وشهورا, في,...\n",
      "3    [المريخ, الكوكب, الأحمر, ثاني, أقرب, الكواكب, ...\n",
      "4    [سقط, الإخوان, في, مصر, بأسرع, مما, يتوقع, أو,...\n",
      "Name: Tokenized_Document, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def tokenize_arabic_text(text):\n",
    "    # if the input is a string\n",
    "    if isinstance(text, str):\n",
    "        # expression pattern to tokenize the text\n",
    "        pattern = r'\\b\\w+\\b'\n",
    "\n",
    "        # Tokenize text using the pattern\n",
    "        tokenizer = nltk.tokenize.RegexpTokenizer(pattern)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "    else:\n",
    "        tokens = []\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to the \"Document\" column\n",
    "df['Tokenized_Document'] = df['Document'].apply(tokenize_arabic_text)\n",
    "print(df['Tokenized_Document'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [كشفت, دراسة, علمية, الرجال, المتزو, جين, يكتس...\n",
      "1    [تزال, عصابات, تهريب, المخدرات, وكل, أنواع, ال...\n",
      "2    [تقضي, الكثير, النساء, أسابيع, وشهورا, استعادة...\n",
      "3    [المريخ, الكوكب, الأحمر, أقرب, الكواكب, إلينا,...\n",
      "4    [سقط, الإخوان, مصر, بأسرع, يتوقع, يتخيل, خصومه...\n",
      "Name: Tokenized_Document, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Load Arabic stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words('arabic'))\n",
    "\n",
    "# Remove stopwords from a list of tokens\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply stopwords removal to the 'Tokenized_Document' column\n",
    "df['Tokenized_Document'] = df['Tokenized_Document'].apply(remove_stopwords)\n",
    "\n",
    "# Print the first 5 rows of the DataFrame with stopwords removed\n",
    "print(df['Tokenized_Document'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [كشف, درس, علم, رجل, تزو, جين, كسب, وزن, قرن, ...\n",
      "1    [تزل, عصب, هرب, خدر, وكل, واع, منع, دمر, جمع, ...\n",
      "2    [تقض, كثر, نسء, ابع, شهر, عاد, ذكر, علق, حب, ف...\n",
      "3    [ريخ, كوكب, حمر, قرب, ككب, الي, جمع, شمس, ربع,...\n",
      "4    [سقط, اخو, مصر, أسرع, وقع, تخل, خصم, لهذ, سقط,...\n",
      "Name: Tokenized_Document, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Load Arabic stemmer\n",
    "stemmer = nltk.stem.ISRIStemmer()\n",
    "\n",
    "# Define a function to perform stemming on a list of tokens\n",
    "def stem_tokens(tokens):\n",
    "    # Stem each token in the list of tokens\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "# Assume that 'Tokenized_Document' column contains tokenized documents with stopwords removed\n",
    "\n",
    "# Apply stemming to the 'Tokenized_Document' column\n",
    "df['Tokenized_Document'] = df['Tokenized_Document'].apply(stem_tokens)\n",
    "\n",
    "# Print the first 5 rows of the DataFrame with stemming applied\n",
    "print(df['Tokenized_Document'].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
